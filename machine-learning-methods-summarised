#There are two main approaches to dealing with the curse of dimensionality. Firstly, data can be projected into a lower dimensional space (generally referred to as “dimensionality reduction”). The lower dimensional space is generally defined by an algorithm to optimally preserve some characteristic(s) of the original data. Since information is always lost during projection, the choice of projection method involves a prioritization of a specific set of properties. Secondly, uninformative genes can be removed, referred to as “feature selection” in machine learning, to reduce the number of dimensions used in the analysis. Reducing the number of genes not only facilitates visualization, but it may also reduce noise and speed up calculations. Below we discuss some of the most popular methods for unsupervised dimensionality reduction and feature selection for scRNASeq data.

3.1. Dimensionality reduction
Principal component analysis (PCA) is a deterministic algorithm which projects data into a reduced number of independent dimensions. Dimensions are linear and they capture the highest variance possible. PCA is relatively fast, and when used with sparse-matrix representations it can scale to very large datasets. PCA generally preserves both long-range and short-range relationships amongst data points. A drawback is that PCA is restricted to linear dimensions and assumes approximately normally distributed data; two assumptions that may not be appropriate for scRNASeq datasets. A variation of PCA which explicitly deals with the large number of zero-values in scRNASeq data has been developed (Pierson and Yau, 2015) but the zero-inflation model employed may not fit all datasets (Andrews and Hemberg, 2016). Recently Risso et al. (2017) proposed a method similar to PCA based on a zero-inflated negative binomial model instead of a Gaussian model.

T-distributed stochastic neighbor embedding (tSNE) is a stochastic method designed for visualizing large high dimensional datasets (Maaten et al., 2008). A two or three dimensional embedding of the high dimensional data which preserves local structure amongst cells is calculated, but as a trade-off long-range information is lost (Fig. 2). Due to the probability distributions used to estimate the embedding, tSNE specifically projects data into isolated clusters, making it a popular choice for visualizing cell populations in scRNASeq data (Baron et al., 2016, Campbell et al., 2017, Macosko et al., 2015, Muraro et al., 2016, Segerstolpe et al., 2016).

A drawback of tSNE is the stochastic nature of the algorithm, and applying tSNE to the same dataset multiple times will produce different embeddings. Although the differences are often small and insignificant, best practice is to run the algorithm multiple times to ensure the stability of results. In addition, tSNE embeddings are sensitive to the choice of the “perplexity” parameter. Thus, it is necessary to run the algorithm multiple times to determine the appropriate perplexity for a particular dataset (Fig. 2C,D). The authors of the method recommend only using tSNE for visualization purposes and not as a dimensionality reduction method (Maaten et al., 2008).

Diffusion maps (DM) is a nonlinear projection method which has predominantly been used for analyzing continuous progressions of cells (Moon et al., 2017, Angerer et al., 2016, Haghverdi et al., 2016). DM is based on models of a diffusion process to embed high dimensional data in low dimensional space. It is assumed that the low dimensional space is smooth and that it can be inferred from the distances between the cells. Unlike tSNE, DMs preserve both local and distant relationships between points. Since DM assumes a relatively smooth continuum of cells it performs well on large RT-qPCR and large scRNASeq experiments (i.e. > 1000 cells assayed) but performance drops for datasets with few cells or the presence of very distinct cell populations (Qiu et al., 2017).

3.2. Feature selection
Michaelis-Menten modelling of dropouts (M3Drop) uses the relatively tight relationship between dropout rate (i.e. the frequency of zeros) and mean expression to perform feature selection. Genes with high dropout rate relative to their expression are likely to be differentially expressed across subpopulations of cells within the dataset. Thus, identifying outliers from the fitted relationship is an effective method of feature selection for scRNASeq, and it can be shown that the method improves clustering and allows for batch effect corrections (Andrews and Hemberg, 2016).

Highly variable genes (HVG) is based on the assumption that genes with high variance relative to their mean expression are due to biological effects rather than just technical noise. The method seeks to identify genes that have a higher variability than expected by considering the relationship between variance and mean expression. This relationship is difficult to fit, and in practice genes are ranked by their distance from a moving median (Kolodziejczyk et al., 2015) or another statistic derived from variance is used, e.g. the squared coefficient of variation (Brennecke et al. (2013)).

Spike-in based methods use a similar idea to HVG and M3Drop to identify features of interest. Here, technical noise is explicitly modelled using data from spike-in RNAs to identify genes exhibiting dropout rates or variance significantly higher than those of spike-ins with similar expression levels. Examples of spike-in based methods include those by Brennecke et al., 2013, BASiCS (Vallejos et al., 2015), and scLVM (Buettner et al., 2015).

Correlated expression is a different approach to identifying biologically relevant genes specifically for identifying cell populations (Andrews and Hemberg, 2016). Genes differentially expressed between a pair of cell-types will be correlated with each other. Correlations will be positive if they are co-expressed in the same cell-type and negative if they are expressed in different cell-types. Feature selection proceeds using either the magnitude and/or significance of the correlations. An alternative which combines high variability and correlation information is to use gene-loading from PCA e.g. (Macosko et al., 2015, Pollen et al., 2014, Usoskin et al., 2015). PAGODA (Fan et al., 2016) performs a variant on this method which combines HVG and PCA loadings to identify important sets of genes which either were highly correlated in the dataset or which share functional annotations.

The methods for dealing with high dimensional data presented here are not mutually exclusive, and it is common practice to apply multiple approaches. Dimensionality reduction methods, i.e. PCA, tSNE, and DM, are susceptible to batch effects and technical noise which may obscure structure within the data (Finak et al., 2015, Hicks et al., 2015, Tung et al., 2017). Performing feature selection to remove genes with little biological signal prior to dimensionality reduction can greatly reduce these effects (Andrews and Hemberg, 2016). Examples of such approaches include iteratively performing spike-in based feature selection followed by PCA (Liu et al., 2016, Tasic et al., 2016), HVG feature selection followed by tSNE (Segerstolpe et al., 2016), and HVG feature selection followed by dimensionality reduction with both PCA and tSNE (Campbell et al., 2017).